%% Copyright (C) 2021 Alessandro Clerici Lorenzini
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Alessandro Clerici Lorenzini
%
% This work consists of the files listed in work.txt


\section{Statistica inferenziale}
La statistica inferenziale tenta di applicare un'inferenza attraverso l'induzione. La statistica inferenziale riprende concetti della statistica descrittiva ma li reinterpreta in senso probabilistico applicando l'induzione.

\subsection{Definizioni}
Gli attori fondamentali della statistica inferenziale sono la popolazione, il campione la statistica o stimatore\footnote{in alcuni contesti, i termini statistica e stimatore differiscono leggermente di significato. Tuttavia in questo testo verranno usati equivalentemente.}, e la stima.

\begin{itemize}
	\item La popolazione è descritta come una \emph{variabile aleatoria $X$}. La branca della statistica inferenziale in cui la distribuzione di $X$ è nota a meno di uno o più parametri $\theta$ si dice \emph{parametrica}: $X\sim F(\theta)$. Se anche il modello della distribuzione è ignoto, si tratta di statistica inferenziale non parametrica. Nel resto di questo testo si studierà la statistica inferenziale parametrica;
	\item Poiché non sempre si vuole ricavare o approssimare il parametro di distribuzione ignoto $\theta$, ma talvolta un valore che ne deriva, si introduce la funzione $\tau(\theta)$ a indicare tale valore;
	\item L'informazione conosciuta sulla popolazione è data da un \emph{campione}, talvolta espresso come una serie di variabili aleatorie $X_1,\dots,X_n$ indipendenti e identicamente distribuite, talvolta come una tupla di loro specificazioni $x_1,\dots,x_n$;
	\item L'operazione di stimare $\tau(\theta)$ consiste nella statistica o stimatore, che è una funzione $t$ che associa a una tupla possibili specificazioni del campione a un numero reale che stima il valore cercato: $t:D_X^n\to\R$. Essendo la statistica una variabile aleatoria, la si rappresenta talvolta (quando si vuole evidenziare tale lettura) con $T$;
	\item Una stima $\hat\tau$ è un'immagine della statistica e approssima $\tau(\theta)$: $\hat\tau = t(x_1,\dots,x_n)$ con istanze $X_1=x_1,\dots,X_n=x_n$.
\end{itemize}

\subsubsection{Stimatori non deviati}
\begin{defin}[stimatore non deviato]
	Uno stimatore $t$ è non deviato per una quantità $\tau(\theta)$ quando il suo valore atteso è uguale a $\tau(\theta)$:
	\begin{equation*}
		\ev{t(X_1,\dots,X_n)} = \tau(\theta)
	\end{equation*}
\end{defin}
Uno stimatore non è deviato quando non è soggetto ad un bias.

\begin{examp}
	Uno stimatore non deviato per stimare il valore atteso, indipendentemente dalla distribuzione, è quello che fa corrispondere alle specificazioni date la loro media:
	\begin{equation*}
		t(x_1,\dots,x_n) = \frac{1}{n} \sum_{i=1}^n x_i \qquad \tau(\theta) = \ev{X}
	\end{equation*}
	Infatti:
	\begin{align*}
		\ev{t} & = \ev{\frac{1}{n} \sum_{i=1}^n X_i} \\
		       & = \frac{1}{n}\sum_{i=1}^n \ev{X_i}  \\
		       & = \frac{1}{n} \cdot n\ev{X_i}       \\
		       & = \ev{X}
	\end{align*}
	Inoltre, calcolando la varianza dello stimatore ci si accorge che essa tende a 0 per campioni molto grandi:
	\begin{equation*}
		\var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n \var(X_i) = \frac{n}{n^2}\var(X) = \frac{\var(X)}{n}
	\end{equation*}
\end{examp}


\subsection{Errore quadratico medio}
Il errore quadratico medio (Mean Square Error o MSE) è un modo di valutare uno stimatore di una quantità ignota:
\begin{defin}[errore quadratico medio]
	\begin{equation*}
		\MSE_{\tau(\theta)}(T) = \ev{(T-\tau(\theta))^2}
	\end{equation*}
	con $T=t(X_1,\dots,X_n)$.
\end{defin}

\begin{defin}
	Il bias di uno stimatore $T$ su $\tau(\theta)$ è la differenza tra il suo valore atteso e $\tau(\theta)$:
	\begin{equation*}
		b_{T(\theta)}(T) = \ev{T} - \tau(\theta)
	\end{equation*}
\end{defin}

In virtù delle definizioni di MSE e bias si può trarre un interessante risultato:
\begin{align*}
	\MSE_{\tau(\theta)}(T) & = \ev{(T-\tau(\theta))^2}                                                                            \\
	                       & = \ev{(T-\ev{T}+\ev{T}-\tau(\theta))^2}                                                              \\
	                       & = \ev{(T-\ev{T})^2+2(T-\ev{T})(\ev{T}-\tau(\theta))+(\ev{T}-\tau(\theta))^2}                         \\
	                       & = \ev{(T-\ev{T})^2} + 2(\ev{T}-\tau(\theta))\underbrace{\ev{T-\ev{T}}}_{0} + (\ev{T}-\tau(\theta))^2 \\
	                       & = \var(T) + (\ev{T}-\tau(\theta))^2                                                                  \\
	                       & = \var(T) + (b_{T(\theta)}(T))^2
\end{align*}
Per definizione, se $T$ è uno stimatore non deviato per $\tau(\theta)$ allora $b_{\tau(\theta)}(T)=0$ e quindi l'errore quadratico medio di $T$ è uguale alla sua varianza.


\subsection{Stimatori consistenti}
\begin{defin}[Stimatore consistente in media quadratica]
	Uno stimatore $T_n$\footnote{Più precisamente, si parla di una famiglia di stimatori $T_n$ i cui membri differiscono per dimensione del campione} è consistente in media quadratica per $\tau(\theta)$ (tau) se
	\begin{equation*}
		\lim_{n\to+\infty} \MSE_{\tau(\theta)}(T_n) = 0
	\end{equation*}
\end{defin}

\begin{defin}[Stimatore debolmente consistente]
	Uno stimatore $T_n=t(X_1,\dots,X_n)$ è debolmente consistente rispetto a una quantità ignota $\tau(\theta)$ se e solo se
	\begin{equation*}
		\forall\varepsilon>0 \quad \lim_{n\to+\infty} P(\tau(\theta)-\varepsilon \leq T_n \leq \tau(\theta)+\varepsilon) = 1
	\end{equation*}
\end{defin}

%\footnotesize{}
%dato un \tau(\theta) fissato, dopo un valore  \epsilon, il valore sarà consistente

\begin{teor}
	Se uno stimatore $T$ è consistente in media quadratica per $\tau(\theta)$ allora $T$ è debolmente consistente per $\tau(\theta)$.
\end{teor}
\begin{proof}
	\begin{align*}
		P(-\varepsilon\leq T_n-\tau(\theta)\leq \varepsilon) & = P(\abs{T_n-\tau(\theta)}\leq\varepsilon)                                                                         \\
		                                                     & = P((T_n-\tau(\theta))^2\leq\varepsilon^2)                                                                         \\
		                                                     & = 1 - P((T_n-\tau(\theta))^2>\varepsilon^2)                                                                        \\
		                                                     & > 1-\frac{\ev{(T_n-\tau(\theta))^2}}{\varepsilon^2} \bc{\parbox{35mm}{disuguaglianza \ref{teor:markov} di Markov}} \\
		                                                     & = 1-\frac{\MSE_{\tau(\theta)}(T_n)}{\varepsilon^2} \to 1
	\end{align*}
	Essendo le probabilità limitate superiormente da $1$, per il teorema del confronto la probabilità iniziale tende a $1$.
\end{proof}


\subsection{Legge dei grandi numeri}
La legge dei grandi numeri forte ci dice che, date $n$ osservazioni con $n$ che tende ad infinito, lo stimatore media campionaria converge quasi certamente (ovvero $P = 1$) al valore atteso comune del $X_i$.
\begin{teor}[legge dei grandi numeri forte]
	\begin{equation*}
		P( \lim_{n\to+\infty} \bar X_n = \mu )= 1
	\end{equation*}
\end{teor}
Così come nella definizione di consistenza, possiamo definire la legge dei grandi numeri debole come:

\begin{teor}[legge dei grandi numeri debole]
	\begin{equation*}
		\forall\varepsilon > 0 \qquad \lim_{n\to+\infty} P\left(\abs{\bar X_n - \mu} > \varepsilon\right) = 0
	\end{equation*}
\end{teor}

%Ragionamento sulla deviazione standard campionaria, e $n-1$
% non si può derivare l'assenza di deviazione di s a partire da s^2

%Consideriamo:
%\begin{center}
%Popolazione $x$ \quad campione $x_1, ..., x_n$ \quad $\ev{x} = \mu$ \quad $var(x) = %\sigma^2$
%\end{center}
%E di voler stimare $\sigma^2$. La varianza campionaria è definita come:
%\begin{equation*}
%s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar x)^2 \quad \ev{s^2} = \sigma^2
%\end{equation*}
%\begin{enumerate}
%\item $\sum_{i=1}^{n} (x_i - \bar x)^2 = \sum_i x_i^2- n\bar x^2$
%\item $\ev{x^2} = var(x) + \ev{x}^2$
%\item $\ev{\bar x} = \mu , var(\bar x) = \frac{\sigma}{n}$
%\end{enumerate}
%...... 




\subsection{Problema dello scarto}
Un problema tipo che fa uso del teorema centrale del limite nella statistica inferenziale è il seguente.

Sia $\bar X_n$ la media campionaria dipendente da $n$ elementi e sia $\mu$ il suo valore atteso. Si vuole che lo scarto $\abs{\bar X_n - \mu}$ tra i due valori sia minore di una soglia $r$ (o solitamente $\epsilon$) con probabilità superiore a $1-\delta$ (con $\delta$ piccolo a piacere):
\begin{equation*}
	P(\abs{\bar X_n - \mu} \leq r) \geq 1-\delta \\
\end{equation*}

Normalizzando:
\begin{align*}
	P\left(\frac{\abs{\bar X_n - \mu}}{\sigma/\sqrt{n}} \leq \frac{r}{\sigma/\sqrt{n}}\right) & = P\left(\abs{\frac{\bar X_n - \mu}{\sigma/\sqrt{n}}} \leq \frac{r\sqrt{n}}{\sigma}\right)               \\
	                                                                                          & \approx P\left(\abs{Z} \leq \frac{r}{\sigma}\sqrt{n}\right)                                              \\
	                                                                                          & = P\left(-\frac{r}{\sigma}\sqrt{n}\leq Z\leq \frac{r}{\sigma}\sqrt{n}\right)                             \\
	                                                                                          & = \Phi\left(\frac{r}{\sigma}\sqrt{n}\right) - \Phi\left(- \frac{r}{\sigma}\sqrt{n}\right)                \\
	                                                                                          & = \Phi\left(\frac{r}{\sigma}\sqrt{n}\right) - \left(1 - \Phi\left(\frac{r}{\sigma}\sqrt{n}\right)\right)
\end{align*}

Ergo:
\begin{align*}
	2\Phi\left(\frac{r}{\sigma}\sqrt{n}\right) - 1 & \geq 1 - \delta                                                              \\
	\Phi\left(\frac{r}{\sigma}\sqrt{n}\right)      & \geq 1 - \frac{\delta}{2}                                                    \\
	\frac{r}{\sigma}\sqrt{n}                       & \geq \Phi^{-1}\left(1-\frac{\delta}{2}\right)                                \\
	n                                              & \geq \left(\frac{\sigma}{r}\Phi^{-1}\left(1-\frac{\delta}{2}\right)\right)^2
\end{align*}

Allo stesso modo si possono ricavare gli altri parametri del problema:
\begin{gather*}
	r \geq \frac{\sigma}{\sqrt{n}}\Phi^{-1}\left(1-\frac{\delta}{2}\right) \\
	\delta \geq 2\left(1-\Phi\left(\frac{r}{\sigma}\sqrt{n}\right)\right)
\end{gather*}
% TODO: verificare che $\sqrt{s^2}$ sia uno stimatore non deviato per $\sigma$
Mentre $\sigma$ tipicamente è stimato come $\sqrt{s^2}$ (dove $s$ è la deviazione standard campionaria).
\subsubsection{Variante con Tchebishev}
Consideriamo una variabile aleatoria $X$, con valore atteso: $\ev{X} = \mu$ e varianza: $var(X) = \sigma^2$:
Un'alternativa a questo tipo di stima si può fare applicando la disuguaglianza di Tchebishev:
\begin{equation*}
 P(\abs{X - \mu} < r) = 1- P(\abs{x-\mu} \geq r) \geq 1 - \frac{\sigma^2}{r^2}
\end{equation*}
% TODO: r=\varepsilon, coerenza a riguardo
% TODO: avere più uniformità tra i due metodi
Ne consegue che:
\begin{align*}
	\forall r > 0 \qquad P(\abs{X-\mu}\geq r)                                                       & \leq \frac{\sigma^2}{r^2}     \\
	P(\abs{X-\mu} < r) = 1-P(\abs{X-\mu}\geq r)                                & \geq 1 - \frac{\sigma^2}{r^2} \\
	P(\abs{\bar X-\mu} < \varepsilon) \geq 1 - \frac{\sigma^2}{n\varepsilon^2} & \geq 1-\delta                 \\
	\frac{\sigma^2}{n\varepsilon^2}                                            & \leq \delta                   \\
\end{align*}
% TODO: spiegazione
\begin{gather*}
	n \geq \frac{\sigma^2}{\delta\varepsilon^2} \Rightarrow P(\abs{\bar X-\mu}<\varepsilon) \geq 1-\delta \\
	\delta\geq\frac{\sigma^2}{n\varepsilon^2} \\[1ex]
	\varepsilon^2 \geq \frac{\sigma^2}{n\varepsilon} \rightarrow \varepsilon \geq \sqrt{\frac{\sigma^2}{n\delta}}
\end{gather*}

\subsubsection*{Per riassumere}
Con teorema centrale del limite: 
\begin{align*}
n 	& \geq \frac{\sigma^2}{\varepsilon^2} \Phi^{-1}(1-\frac{\delta}{2})^2 \\
 	& \geq \frac{\sigma^2}{\varepsilon^2} \Phi^{-1} (1-\frac{\delta}{2})^2 < \frac{\sigma^2}{\delta \varepsilon^2}\\
	& \geq \Phi^{-1} (1-\frac{\delta}{2})^2 < \frac{\sigma^2}{\delta}
\end{align*}
con Tchebishev:
\begin{align*}
	n & \geq \frac{\sigma^2}{\delta \varepsilon^2}\\
	 & \geq \Phi^{-1} ( 1 - \frac{\delta}{2})^2 < \frac{1}{\delta}
\end{align*}

Il metodo di Tchebishev è preferibile quando si vuole evitare approssimazioni, tuttavia rappresenta una approssimazione più lasca rispetto al metodo con teorema centrale del limite in quanto deve essere sempre valido.

% TODO: tutta la sezione sul processo di Poisson va rivista.
% TODO: definizione di processo stocastico

\subsection{Processo di Poisson}
Il processo di Poisson dimostra che sotto certe ipotesi il problema del numero di eventi che avvengono in un certo intervallo di tempo è descritto da un modello di Poisson.

\subsubsection{Processo stocastico}
\begin{teor}
	Sia $t$ una variabile temporale e $N(t)$ la variabile aleatoria che esprime il numero di eventi che avvengono nell'intervallo di tempo $[0,t)$. 
	\end{teor}
Possiamo dire che \begin{equation*}
	\{N(t): t > 0\} 
	\end{equation*}
	
	rappresenta un processo stocastico. Se estendiamo questa definizione con le seguenti ipotesi, possiamo dire che è anche un "processo di Poisson":\begin{center} con $\lambda > 0$
	\end{center}
	\begin{enumerate}
		\item $N(0)=0$;
		\item istanze di $N$ sono indipendenti per intervalli disgiunti, di conseguenza sono anche variabili aleatore indipendenti;
		\item la distribuzione del numero degli eventi dipende solo dalla lunghezza dell'intervallo di tempo (e non dalla loro posizione);
		      % TODO: spiegare la versione approssimata dell'ipotesi 3
		\item $\displaystyle\lim_{h\to0}\frac{P(N(h)=1)}{h}=\lambda$;\footnote{di conseguenza se $h$ è "piccolo" : $P(N(h) = 1) = \lambda h$}
		\item $\displaystyle\lim_{h\to0}\frac{P(N(h)\geq 2)}{h}=0$.\footnote{per $^6$, $P(N(h) \geq 2) = 0$}


	\end{enumerate}
	allora $N(t)$ è distribuita secondo il modello di Poisson con parametro $\lambda t$:
	\begin{equation*}
		N(t)\sim P(\lambda t)
	\end{equation*}

\begin{proof}
	Volendo calcolare la massa di probabilità $P(N(t)=k)$, si sceglie $n\in\N$ e si divide l'intervallo $[0,t)$ in $n$ parti uguali $[\frac{m}{n}t,\frac{m+1}{n}t)$. Potendo scegliere $n$ grande a piacere si può scegliere in particolare $n>k$.

	L'evento $N(t)=k \in \mathbb{N}$, di cui si vuole calcolare la probabilità, è esprimibile come l'unione disgiunta di due eventi:
	\begin{itemize}
		\item $A$: ognuno dei $k$ eventi avviene in un intervallo diverso, ossia: ogni intervallo contiene al più un evento, per un totale di $k$ eventi;
		\item $B$: esiste almeno un intervallo che contiene due eventi, per un totale di $k$ eventi.
	\end{itemize}
	e quindi:
	\begin{equation*}
		N(t)=k \in \mathbb{N} \leftrightarrow (A) \cup (B)
	\end{equation*}
	Ricordando che $n$ è grande a piacere, per l'ipotesi 4 la probabilità che uno specifico intervallo contenga due eventi tende a $0$. 
	Inoltre, per l'ipotesi 2 la probabilità dell'evento $B$ è calcolabile come il prodotto delle singole. 
	
	La probabilità complessiva è quindi il prodotto di fattori tendenti a $0$ ed è pertanto tendente a $0$ per $n\to+\infty$. Si ha quindi:
	\begin{align*}
		P(N(t)=k) & = P (A \cup B) = P(A) +  \xcancel{P(B)}\\
				& = \binom{n}{k} (\lambda \frac{t}{n})^k (1 - \lambda \frac{t}{n})^{n-k}
	\end{align*}
e quindi:
\begin{equation*}
	N(t) \sim B(n, \frac{\lambda t}{n})
\end{equation*}


Poiché il prodotto dei parametri della binomiale trovata è costante, ed essendo $n\to+\infty$, per quanto visto al paragrafo \ref{subsub:binompois} la distribuzione è ben descritta da un modello di Poisson di parametro $\lambda t$:
	\begin{equation*}
		N(t) \sim P(\lambda t)
	\end{equation*}

Per riassumere, con $n \rightarrow \infty$:
\begin{equation*}
	P(N(t) = k) = \frac{(\lambda t)^k}{k!} e^{-\lambda t}\quad I_{\mathbb{N} \cup \{0\}^{(k)}} 
\end{equation*}

%Inoltre:
%\begin{equation*}
 %P(nessun evento in (s, s+t\] = P (nessun evento in (0, t\] = 
 %P(N(t) = 0) = e^{-\lambda t}
%\end{equation*}

-----------


	Siano $B_i$ le variabili aleatorie bernoulliane che descrivono l'esistenza o meno di un evento nell'$i$-esimo intervallo. Per l'ipotesi 3 si ha, per $n\to+\infty$:
	\begin{equation*}
		B_i \sim B\left(\lambda\frac{t}{n}\right)
	\end{equation*}
	Si noti inoltre che l'evento $A$ corrisponde a una particolare specificazione della variabile aleatoria binomiale $B'$ che descrive il numero di eventi complessivi in $n$ intervalli, ossia la somma dei $B_i$:
	\begin{equation*}
		B'\sim B\left(n, \lambda\frac{t}{n}\right)
	\end{equation*}

	Ma allora:
	\begin{gather*}
		P(N(t)=k) = P(A) = P(B'=k) \\
		N(t) = B'
	\end{gather*}

	
	% TODO: esponenziale per il tempo trascorso da un evento all'altro
\end{proof}
