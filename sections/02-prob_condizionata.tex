%% Copyright (C) 2021 Alessandro Clerici Lorenzini
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Alessandro Clerici Lorenzini
%
% This work consists of the files listed in work.txt


\section{Probabilità condizionata}
La probabilità condizionata vede protagonisti un evento condizionato e un evento condizionante e, concettualmente, restringe lo spazio campionario all'evento condizionante.

\begin{defin}
	La probabilità condizionata da un evento $F$ (evento condizionante) su un evento $E$ (evento condizionato) viene definita, quando $F$ non è impossibile (ossia per $P(F)\neq 0$), come segue:
	\begin{equation*}
		P(E\mid F)=\frac{P(E \cap F)}{P(F)}
	\end{equation*}
\end{defin}


Dalla definizione di probabilità condizionata deriva la seguente \textbf{regola di fattorizzazione}:
\begin{equation} \label{eq:regfatt}
	P(E \cap F)=P(E\mid F)\cdot P(F)
\end{equation}



\subsection{Teorema delle probabilità totali}

\subsubsection{Caso particolare}
Dati eventi $E, F\in \A$, l'evento $E$ può essere scritto come la parte di $E$ che non contiene elementi di $F$, ossia $E\cap \bar F$, unita disgiuntamente alla parte che ne contiene, ossia $E \cap F$:
\begin{align*}
	(E \cap \bar F) \cap (E \cap F) & = E \cap (F \cap \bar F)\bc{proprietà distributiva}  \\
	                                & = E \cap \emptyset = \emptyset                       \\[2ex]
	(E \cap \bar F) \cup (E \cap F) & = E \cap (F \cup \bar F) \bc{proprietà distributiva} \\
	                                & = E \cap \Omega = E
\end{align*}

Avendo espresso $E$ come unione di eventi disgiunti, si può applicare il terzo assioma:
\begin{equation*}
	P(E) = P((E \cap \bar F)\cup(E \cap F)) = P(E \cap \bar F)+P(E \cap F)
\end{equation*}
e, per la regola \eqref{eq:regfatt} di fattorizzazione, assunto $P(F)\neq 0 \land P(\bar F)\neq 0$:
\begin{equation*}
	=P(E\mid \bar F)\cdot P(\bar F) + P(E \mid F)\cdot P(F)
\end{equation*}

\subsubsection{Forma generale}
\begin{teor}[delle probabilità totali]
	Data una partizione di $\Omega$ composta da eventi non impossibili $F_1,\dots, F_n$ e un evento $E$:
	\begin{equation*}
		P(E)= \sum_{i=1}^n P(E \mid F_i)\cdot P(F_i)
	\end{equation*}
\end{teor}
\begin{proof}
	La dimostrazione avviene in analogia con il caso particolare. Questa volta, tuttavia, gli eventi della partizione sono disgiunti per ipotesi e non in quanto complementari:
	\begin{equation*}
		P(E)=P \left(\bigcup_{i=1}^n (E\cap F_i) \right) = \sum_{i=1}^n P(E\cap F_i) = \sum_{i=1}^n P(E \mid F_i)\cdot P(F_i) \qedhere
	\end{equation*}
\end{proof}


%1:13:15

\subsection{Teorema di Bayes}
Nelle stesse ipotesi del teorema delle probabilità totali si dimostra un altro teorema, detto di Bayes:
\begin{teor}[di Bayes]
	Data una partizione di $\Omega$ composta da eventi non impossibili $F_1,\dots, F_n$ e un evento $E$:
	\begin{equation*}
		P(F_j\mid E)=\frac{P(E\mid F_j)\cdot P(F_j)}{\sum\limits_{i=1}^n P(E \mid F_i)\cdot P(F_i)}
	\end{equation*}
\end{teor}

\begin{proof}
	\begin{equation*}
		P(F_j\mid E)=\frac{P(F_j \cap E)}{P(E)}=\frac{P(E\mid F_j)\cdot P(F_j)}{\sum\limits_{i=1}^n P(E \mid F_i)\cdot P(F_i)} \qedhere
	\end{equation*}
\end{proof}



\subsection{Eventi indipendenti}
\begin{defin}
	Eventi $E$ e $F$, con $P(F)>0$, si dicono indipendenti se $P(E \mid F)=P(E)$
\end{defin}

Equivalentemente, per definizione di probabilità condizionata
\begin{gather*}
	\frac{P(E \cap F)}{P(F)}=P(E) \\[1ex]
	P(E \cap F)=P(E)\cdot P(F)
\end{gather*}
e ovviamente, se $P(E)>0$:
\begin{equation*}
	P(F \mid E)=P(F)
\end{equation*}

\subsubsection{Proprietà}
\begin{teor}
	Se $E$ e $F$ sono indipendenti, allora $E$ e $\bar F$ sono indipendenti.
\end{teor}
\begin{proof}
	\begin{equation*}
		E = (E\cap F) \cup (E \cap \bar F)
	\end{equation*}
	\begin{align*}
		P(E \cap \bar F) & = P(E) - P(E \cap F)    \bc{terzo assioma}                          \\
		                 & = P(E) - P(E)\cdot P(F) \bc{in quanto $E$ ed $F$ sono indipendenti} \\
		                 & = P(E)(1-P(F))                                                      \\
		                 & = P(E)\cdot P(\bar F)   \bc{teorema \ref{t:probcompl}} \qedhere
	\end{align*}
\end{proof}

\subsubsection{Tripla di eventi indipendenti}
\begin{defin}
	Tre eventi $E$, $F$, $G$ sono indipendenti se
	\begin{itemize}
		\item $P(E \cap F)=P(E)\cdot P(F)$
		\item $P(E \cap G)=P(E)\cdot P(G)$
		\item $P(F \cap G)=P(F)\cdot P(G)$
		\item $P(E \cap F \cap G)=P(E)\cdot P(F)\cdot P(G)$
	\end{itemize}
\end{defin}

\noindent
Per quanto riguarda $E$, $F\cup G$:
\begin{align*}
	P(E \cap & (F\cup G))  = P((E\cap F)\cup (E\cap G))                \bc{distributiva}              \\
	         & = P(E \cap F) + P(E\cap G) - P((E\cap F)\cap (E\cap G)) \bc{teorema \ref{t:probunion}} \\
	         & = P(E)P(F) + P(E)P(G) - P(E)P(F)P(G)                    \bc{indipendenza}              \\
	         & = P(E)(P(F) + P(G)-P(F)P(G))
\end{align*}

\subsubsection{\texorpdfstring{$n$}{n}-upla di eventi indipendenti}
\begin{defin}[$n$-upla di eventi indipendenti]
	Eventi $E_1,\dots,E_n$ si dicono indipendenti se e solo se comunque apportata su di essi una selezione di $r$ eventi la probabilità dell'intersezione degli eventi selezionati è uguale al prodotto della probabilità dei singoli.
	\begin{equation*}
		\forall r<n\in\mathbb{N}~\forall~1\leq \alpha_1 \leq\dots\leq\alpha_r\leq n \qquad P \left(\bigcap_{i=1}^r E_{\alpha_i} \right)=\prod_{i=1}^r P(E_{\alpha_i})
	\end{equation*}
\end{defin}

\begin{examp}
	Dato un sistema in serie di dimensione $n$:
	\begin{gather*}
		\forall i=1,\dots,n \qquad p_i=P(\text{l'}i\text{-esimo componente funziona}) \\[1ex]
		P(\text{il sistema funziona}) = P(\text{tutti i componenti funzionanno}) = \\
		P\left(\bigcap_{i=1}^n \text{l'}i\text{-esimo componente funziona}\right)
	\end{gather*}

	In un sistema in parallelo:
	\begin{gather*}
		P(\text{il sistema funziona}) = 1-P(\text{il sistema non funziona}) = \\[1ex]
		1-P(\text{tutti i componenti non funzionano}) = \\[1ex]
		1- P\left(\bigcap_{i=1}^n \text{l'}i\text{-esimo componente non funziona}\right) = \\[1ex]
		1-\prod_{i=1}^n P(\text{l'}i\text{-esimo componente non funziona}) =
		1-\prod_{i=1}^n 1-p_i =
	\end{gather*}
\end{examp}
